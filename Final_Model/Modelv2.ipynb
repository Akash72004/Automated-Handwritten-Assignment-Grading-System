{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pdf2image pillow numpy nltk bert-score openai"
      ],
      "metadata": {
        "id": "9ZVbYYrT6e8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5e12e3-7374-4050-fbc1-7f0c0053d6cd"
      },
      "id": "9ZVbYYrT6e8_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0865c6-670e-406d-80a4-e8323c4bfcfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d0865c6-670e-406d-80a4-e8323c4bfcfa",
        "outputId": "22e1ef9a-8ea2-4272-fb9c-df956ff19d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports successful.\n",
            "BERTScore version: 0.3.12\n",
            "OpenRouter client initialized.\n",
            "Using VL Model: meta-llama/llama-4-maverick:free\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 1: Imports and Setup\n",
        "# -------------------------------\n",
        "import os\n",
        "import re\n",
        "import base64\n",
        "import json\n",
        "import time\n",
        "import functools\n",
        "import traceback\n",
        "\n",
        "from pdf2image import convert_from_path, pdf2image # Import specific exception if needed\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# NLP and Grading specific imports\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "# Ensure necessary NLTK data is downloaded (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "# Note: 'punkt_tab' was in the original grading code imports, but might not be standard.\n",
        "# If errors occur, uncomment the next lines:\n",
        "# try:\n",
        "#     nltk.data.find('tokenizers/punkt_tab')\n",
        "# except nltk.downloader.DownloadError:\n",
        "#     nltk.download('punkt_tab')\n",
        "\n",
        "# Sentence Transformers (if used directly, though BERTScore is primary here)\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# BERTScore for evaluation\n",
        "# Make sure bert-score is installed: pip install bert-score\n",
        "import bert_score # Main import\n",
        "# from bert_score import score as bert_score_calc # Can use alias if preferred\n",
        "\n",
        "# External API Client (OpenRouter in this case)\n",
        "from openai import OpenAI # Use OpenAI client library for compatible APIs\n",
        "\n",
        "# Environment Variables for API Key\n",
        "\n",
        "print(\"Imports successful.\")\n",
        "print(f\"BERTScore version: {bert_score.__version__}\")\n",
        "\n",
        "# --- Environment Setup ---\n",
        "# Load API key from .env file if present\n",
        "# IMPORTANT: Set your API Key here or in a .env file (OPENROUTER_API_KEY=sk-...)\n",
        "OPENROUTER_API_KEY = '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' # Replace default if needed\n",
        "# Initialize API Client (using OpenAI library structure for OpenRouter)\n",
        "# New\n",
        "try:\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=OPENROUTER_API_KEY\n",
        "    )\n",
        "    # Optional: Test connection if API provides a simple ping/model list endpoint\n",
        "    # client.models.list() # Example check if available\n",
        "    print(\"OpenRouter client initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenRouter client: {e}\")\n",
        "    client = None # Set client to None if initialization fails\n",
        "\n",
        "# --- Model Configuration ---\n",
        "# New - Define the VL model to use (as specified in original code)\n",
        "VL_MODEL_NAME = \"meta-llama/llama-4-maverick:free\" # \"moonshot/moonshot-v1-128k\" was another option, stick to original unless specified\n",
        "print(f\"Using VL Model: {VL_MODEL_NAME}\")\n",
        "\n",
        "# Ensure poppler is installed and in PATH for pdf2image, especially on Windows\n",
        "# You might need to specify the path explicitly:\n",
        "# POPPLER_PATH = r\"C:\\path\\to\\poppler-xx.xx.x\\bin\" # Example Windows path\n",
        "# if os.name == 'nt' and 'POPPLER_PATH' in locals():\n",
        "#     print(f\"Setting poppler path to: {POPPLER_PATH}\")\n",
        "#     os.environ[\"PATH\"] += os.pathsep + POPPLER_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "236aaebd-a9d9-46c7-8419-c5a00997fc8d",
      "metadata": {
        "id": "236aaebd-a9d9-46c7-8419-c5a00997fc8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aedd4bb-64b6-4903-d5d1-eb551dace560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded.\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 2: Configuration - Input Files and Grading Parameters\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# --- Input File Paths ---\n",
        "# Path to the question paper (must be an image file like JPEG, PNG)\n",
        "QUESTION_PAPER_PATH = \"/content/question_paper.jpeg\" # Replace with your actual path\n",
        "\n",
        "# Path to the student's answer sheet (can be PDF or an image file/list of image files)\n",
        "ANSWER_SHEET_PATH = \"/content/Joel-D041.pdf\" # Replace with your actual path (PDF or image)\n",
        "# Example for multiple images:\n",
        "# ANSWER_SHEET_PATH = [\"page1.jpg\", \"page2.jpg\"]\n",
        "\n",
        "# Path to the answer key (optional, can be PDF, image, list of images, or 'dict')\n",
        "ANSWER_KEY_PATH = \"/content/Spam_AnswerKey.pdf\" # Replace with actual path or set ANSWER_KEY_SOURCE to 'dict'\n",
        "# Example for multiple images:\n",
        "# ANSWER_KEY_PATH = [\"key_page1.jpg\", \"key_page2.jpg\"]\n",
        "\n",
        "# Define how the Answer Key is provided:\n",
        "# 'file': Process ANSWER_KEY_PATH using VL model OCR.\n",
        "# 'dict': Use the predefined dictionary ANSWER_KEY_DICT below.\n",
        "ANSWER_KEY_SOURCE = 'file' # Change to 'file' if using a file\n",
        "\n",
        "# --- Predefined Answer Key (if ANSWER_KEY_SOURCE = 'dict') ---\n",
        "# Keys MUST be integers corresponding to question numbers.\n",
        "ANSWER_KEY_DICT = {\n",
        "    1: \"\"\"\n",
        "Asymptotic notations are mathematical tools used to describe the running time or space complexity of algorithms:\n",
        "\n",
        "1. Big-O Notation (O):\n",
        "   - Represents the upper bound/worst-case complexity\n",
        "   - f(n) = O(g(n)) means f(n) grows no faster than g(n)\n",
        "   - Example: Bubble Sort has O(n²) in worst case\n",
        "\n",
        "2. Omega Notation (Ω):\n",
        "   - Represents the lower bound/best-case complexity\n",
        "   - f(n) = Ω(g(n)) means f(n) grows at least as fast as g(n)\n",
        "   - Example: Bubble Sort has Ω(n) when array is already sorted\n",
        "\n",
        "3. Theta Notation (Θ):\n",
        "   - Represents both upper and lower bounds (tight bound)\n",
        "   - f(n) = Θ(g(n)) means f(n) grows at the same rate as g(n)\n",
        "   - Example: Matrix multiplication has Θ(n³)\n",
        "\n",
        "Key Points:\n",
        "- Big-O is most commonly used for worst-case analysis\n",
        "- Ω is useful for understanding best-case scenarios\n",
        "- Θ gives precise average-case analysis for balanced algorithms\n",
        "\"\"\",\n",
        "    2: \"\"\"\n",
        "Operations on Circular Linked List:\n",
        "\n",
        "1. Insertion at Start:\n",
        "   newNode = createNode(data)\n",
        "   if head == NULL:\n",
        "       head = newNode\n",
        "       newNode.next = head\n",
        "   else:\n",
        "       temp = head\n",
        "       while temp.next != head:\n",
        "           temp = temp.next\n",
        "       temp.next = newNode\n",
        "       newNode.next = head\n",
        "       head = newNode\n",
        "\n",
        "2. Deletion of Last Node:\n",
        "   if head == NULL: return\n",
        "   if head.next == head:\n",
        "       free(head)\n",
        "       head = NULL\n",
        "   else:\n",
        "       current = head\n",
        "       while current.next.next != head:\n",
        "           current = current.next\n",
        "       free(current.next)\n",
        "       current.next = head\n",
        "\n",
        "Example:\n",
        "Initial: 1 → 2 → 3 → (back to 1)\n",
        "After inserting 0 at start: 0 → 1 → 2 → 3 → (back to 0)\n",
        "After deleting last node: 0 → 1 → 2 → (back to 0)\n",
        "\"\"\",\n",
        "    3: \"\"\"\n",
        "Infix to Postfix Conversion Table for: A + B * (C - D) ^ E / (F - G + H * I)\n",
        "\n",
        "| Symbol | Stack (bottom to top) | Output | Action Taken |\n",
        "|--------|-----------------------|--------|--------------|\n",
        "| A      | []                    | A      | Output operand |\n",
        "| +      | [+]                   | A      | Push operator |\n",
        "| B      | [+]                   | AB     | Output operand |\n",
        "| *      | [+, *]                | AB     | Push operator (higher precedence than +) |\n",
        "| (      | [+, *, (]             | AB     | Push parenthesis |\n",
        "| C      | [+, *, (]             | ABC    | Output operand |\n",
        "| -      | [+, *, (, -]          | ABC    | Push operator |\n",
        "| D      | [+, *, (, -]          | ABCD   | Output operand |\n",
        "| )      | [+, *]                | ABCD-  | Pop until '(' |\n",
        "| ^      | [+, *, ^]             | ABCD-  | Push operator (highest precedence) |\n",
        "| E      | [+, *, ^]             | ABCD-E | Output operand |\n",
        "| /      | [+, *, /]             | ABCD-E^| Pop ^ (higher precedence), then push / |\n",
        "| (      | [+, *, /, (]          | ABCD-E^| Push parenthesis |\n",
        "| F      | [+, *, /, (]          | ABCD-E^F | Output operand |\n",
        "| -      | [+, *, /, (, -]       | ABCD-E^F | Push operator |\n",
        "| G      | [+, *, /, (, -]       | ABCD-E^FG | Output operand |\n",
        "| +      | [+, *, /, (, +]       | ABCD-E^FG- | Pop -, push + (same precedence) |\n",
        "| H      | [+, *, /, (, +]       | ABCD-E^FGH | Output operand |\n",
        "| *      | [+, *, /, (, +, *]    | ABCD-E^FGH | Push operator (higher precedence than +) |\n",
        "| I      | [+, *, /, (, +, *]    | ABCD-E^FGHI | Output operand |\n",
        "| )      | [+, *, /]             | ABCD-E^FGHI*+- | Pop until '(' |\n",
        "| End    | []                    | ABCD-E^FGHI*+-/ | Pop all remaining operators |\n",
        "\n",
        "Final Postfix Expression: ABCD-E^*FGHI*+-/+\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# --- Grading Rubrics/Marks ---\n",
        "# Keys MUST be integers corresponding to question numbers.\n",
        "MARKS_PER_QUESTION = {\n",
        "    1: 3,\n",
        "    2: 7,\n",
        "    3: 5,\n",
        "    # Add max marks for all questions evaluated\n",
        "}\n",
        "\n",
        "# --- PDF to Image Conversion Output Folder ---\n",
        "# New - Made configurable\n",
        "IMAGE_OUTPUT_FOLDER = \"temp_images_output\"\n",
        "\n",
        "# --- Validation ---\n",
        "# Check if input files exist (basic check)\n",
        "if not os.path.exists(QUESTION_PAPER_PATH):\n",
        "    print(f\"ERROR: Question paper image not found at: {QUESTION_PAPER_PATH}\")\n",
        "# Add checks for ANSWER_SHEET_PATH and ANSWER_KEY_PATH if ANSWER_KEY_SOURCE=='file'\n",
        "# ... (add checks as needed)\n",
        "\n",
        "print(\"Configuration loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c2f6cb-5e07-40ab-a065-7fceaaf780d4",
      "metadata": {
        "id": "26c2f6cb-5e07-40ab-a065-7fceaaf780d4"
      },
      "outputs": [],
      "source": [
        "# Code Cell 3: Helper Functions (PDF Conversion, Synonyms, Similarity)\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# From OCRandSegment.ipynb\n",
        "def pdf_to_images(pdf_path, output_folder=\"images\"):\n",
        "    \"\"\"Convert PDF pages to image files.\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"Error: PDF file not found at {pdf_path}\")\n",
        "        return []\n",
        "    try:\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        print(f\"Converting PDF '{os.path.basename(pdf_path)}' to images in '{output_folder}'...\")\n",
        "        # Note: Explicitly passing poppler_path might be needed on some systems\n",
        "        # images = convert_from_path(pdf_path, poppler_path=POPPLER_PATH)\n",
        "        images = convert_from_path(pdf_path)\n",
        "        image_paths = []\n",
        "        for i, image in enumerate(images):\n",
        "            path = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(pdf_path))[0]}_page_{i+1}.jpg\")\n",
        "            image.save(path, \"JPEG\")\n",
        "            image_paths.append(path)\n",
        "        print(f\"Converted PDF to {len(image_paths)} image(s).\")\n",
        "        return image_paths\n",
        "    # except pdf2image.exceptions.PDFInfoNotInstalledError:\n",
        "    #     print(\"ERROR: pdfinfo command not found. Ensure Poppler is installed and in your PATH.\")\n",
        "    #     print(\"See: https://pdf2image.readthedocs.io/en/latest/installation.html\")\n",
        "    #     return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting PDF '{pdf_path}' to images: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return []\n",
        "\n",
        "# From Marks_Accuracy_model1.ipynb\n",
        "@functools.lru_cache(maxsize=128)\n",
        "def get_synonyms(word):\n",
        "    \"\"\"Get synonyms for a given word with caching for performance.\"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
        "    return synonyms\n",
        "\n",
        "# From Marks_Accuracy_model1.ipynb\n",
        "def similar(a, b):\n",
        "    \"\"\"Check if two strings are similar using multiple metrics.\"\"\"\n",
        "    a, b = a.lower(), b.lower()\n",
        "\n",
        "    # Direct match or substring check\n",
        "    if a == b or a in b or b in a:\n",
        "        return True\n",
        "\n",
        "    # Character similarity (Jaccard similarity) - prevent division by zero\n",
        "    a_chars, b_chars = set(a), set(b)\n",
        "    intersection = len(a_chars.intersection(b_chars))\n",
        "    union = len(a_chars.union(b_chars))\n",
        "    if union == 0:\n",
        "      return True if intersection == 0 else False # Both empty strings are similar\n",
        "\n",
        "    return intersection / union > 0.7 # Threshold from original code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "117a1ab1-043e-4d1e-9e76-554055703ebe",
      "metadata": {
        "id": "117a1ab1-043e-4d1e-9e76-554055703ebe"
      },
      "outputs": [],
      "source": [
        "# Code Cell 4: Vision Language Model (VL) OCR and Structuring Functions\n",
        "# -----------------------------------------------------------------------\n",
        "# Based on OCRandSegment.ipynb, using the configured VL Model via OpenRouter client\n",
        "\n",
        "# From OCRandSegment.ipynb\n",
        "def extract_text_from_question_paper(image_path):\n",
        "    \"\"\"\n",
        "    Use VL model to extract text from question paper image with specific formatting.\n",
        "    Returns structured question paper text or None on error.\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        print(\"ERROR: OpenRouter client not initialized. Cannot call API.\")\n",
        "        return None\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"ERROR: Question paper image not found at {image_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Extracting text from Question Paper: {os.path.basename(image_path)} using {VL_MODEL_NAME}...\")\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            base64_image = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "        # Using the specific prompt from the original code\n",
        "        completion = client.chat.completions.create(\n",
        "            model=VL_MODEL_NAME,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": (\n",
        "                                \"You are an OCR expert specialized in academic question papers. \"\n",
        "                                \"Extract all the visible text from this question paper image exactly as it appears. \"\n",
        "                                \"Follow these specific guidelines:\\n\"\n",
        "                                \"1. Start with 'QUESTIONS:'\\n\"\n",
        "                                \"2. For each question, start with 'QUESTION X:' where X is the question number\\n\"\n",
        "                                \"3. Include marks in square brackets [X] where present\\n\"\n",
        "                                \"4. Preserve 'OR' sections exactly as they appear\\n\"\n",
        "                                \"5. Maintain all mathematical expressions, symbols, and formatting\\n\"\n",
        "                                \"6. Do not add any explanations or interpretations\\n\"\n",
        "                                \"7. Do not include any thought process or internal reasoning\\n\"\n",
        "                                \"8. Preserve exact spacing and line breaks between questions\\n\"\n",
        "                                \"9. Extract any instructions or notes exactly as they appear\\n\"\n",
        "                                \"10. Maintain sub-points and numbering exactly as shown\"\n",
        "                            )\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"} # Assuming JPEG/PNG common\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        extracted_text = completion.choices[0].message.content\n",
        "\n",
        "        # Clean up (same as original code) - maybe rename clean_vl_output later if needed\n",
        "        cleaned_text = (\n",
        "            extracted_text\n",
        "            .replace(\"◁think▷\", \"\")  # Remove any thinking markers if present\n",
        "            .replace(\"\\n\\n\\n\", \"\\n\\n\") # Reduce excessive line breaks\n",
        "            .strip()\n",
        "        )\n",
        "\n",
        "        # Ensure the text starts with \"QUESTIONS:\" (same as original)\n",
        "        if not cleaned_text.startswith(\"QUESTIONS:\"):\n",
        "             cleaned_text = \"QUESTIONS:\\n\\n\" + cleaned_text # Add prefix if missing\n",
        "\n",
        "        print(\"Question paper text extracted successfully.\")\n",
        "        return cleaned_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR extracting text from question paper '{os.path.basename(image_path)}': {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# From OCRandSegment.ipynb\n",
        "def extract_text_from_images(image_paths, context_question_paper_text):\n",
        "    \"\"\"\n",
        "    Use VL model to extract text from a list of answer sheet images,\n",
        "    using the question paper text as context.\n",
        "    Returns structured answer text or None on error.\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        print(\"ERROR: OpenRouter client not initialized. Cannot call API.\")\n",
        "        return None\n",
        "    if not image_paths:\n",
        "        print(\"ERROR: No image paths provided for text extraction.\")\n",
        "        return None\n",
        "    if not context_question_paper_text:\n",
        "        print(\"WARNING: Question paper text context is missing. Extraction quality may be reduced.\")\n",
        "        context_question_paper_text = \"QUESTIONS:\\n[Context not available]\" # Provide placeholder\n",
        "\n",
        "    print(f\"Extracting text from {len(image_paths)} answer image(s) using {VL_MODEL_NAME}...\")\n",
        "\n",
        "    # Derive valid question numbers from the context (QP text)\n",
        "    valid_questions = set()\n",
        "    # Improved regex to handle various Q formats like \"QUESTION 1:\", \"Q. 2\", \"Q3)\" etc.\n",
        "    question_matches = re.finditer(r'(?:QUESTION|Q\\.?)\\s*(\\d+[a-zA-Z]?)\\s*[:.)]?', context_question_paper_text, re.IGNORECASE)\n",
        "    for match in question_matches:\n",
        "         q_num_str = match.group(1)\n",
        "         # Try converting to int, handle sub-questions like '3a' if needed later\n",
        "         try:\n",
        "            # For now, just store the string identifier found\n",
        "            valid_questions.add(q_num_str)\n",
        "         except ValueError:\n",
        "            print(f\"Warning: Could not parse question identifier '{q_num_str}' as integer.\")\n",
        "            valid_questions.add(q_num_str) # Keep non-integer identifiers as strings\n",
        "\n",
        "    print(f\"Derived valid question identifiers from QP: {sorted(list(valid_questions))}\")\n",
        "    if not valid_questions:\n",
        "        print(\"WARNING: No valid question identifiers found in the question paper text. Segmentation might fail.\")\n",
        "        # Decide how to proceed: maybe attempt extraction without filtering? Or stop?\n",
        "        # For now, proceed but the VL model might struggle without valid Q numbers.\n",
        "\n",
        "    # Prepare image contents for the API call\n",
        "    image_contents = []\n",
        "    for idx, image_path in enumerate(image_paths):\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"WARNING: Image file not found: {image_path}. Skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            with open(image_path, \"rb\") as img_file:\n",
        "                base64_image = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "            image_contents.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"} # Assuming JPEG/PNG\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading or encoding image {image_path}: {e}\")\n",
        "\n",
        "    if not image_contents:\n",
        "        print(\"ERROR: No valid images could be prepared for API call.\")\n",
        "        return None\n",
        "\n",
        "    # Create the prompt using the structure from original code\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": (\n",
        "                        \"You are an OCR expert specialized in academic answer papers. \"\n",
        "                        f\"You will be analyzing {len(image_contents)} images that may contain parts of the same answers. \"\n",
        "                        \"\\nEXTRACTION RULES:\"\n",
        "                        \"\\n1. Valid question numbers/identifiers from question paper: \" + \", \".join(sorted(list(valid_questions))) + # Use derived identifiers\n",
        "                        f\"\\n\\nReference Question Paper Context:\\n{context_question_paper_text}\\n\" # Provide QP context\n",
        "                        \"\\n2. Multi-Image Processing Rules:\"\n",
        "                        \"\\n   - Combine content from all images to create complete answers for each question number/identifier.\"\n",
        "                        \"\\n   - Avoid duplicating answers for the same question number/identifier.\"\n",
        "                        \"\\n   - Maintain chronological order of content as it appears across images.\"\n",
        "                        \"\\n   - Ensure seamless integration of content belonging to the same answer, even if split across images.\"\n",
        "                        \"\\n3. Text Extraction Guidelines:\"\n",
        "                        \"\\n   - Start each unique answer with 'QUESTION X:' (where X is one of the valid numbers/identifiers listed above).\"\n",
        "                        \"\\n   - Each valid question number/identifier should appear at most once in the final output.\"\n",
        "                        \"\\n   - Maintain consistent formatting for the extracted answer text.\"\n",
        "                        \"\\n   - Preserve all mathematical expressions, code blocks, tables, and symbols exactly as they appear in the answer.\"\n",
        "                        \"\\n4. Critical Rules:\"\n",
        "                        \"\\n   - ONLY structure output using the valid question numbers/identifiers provided.\"\n",
        "                        \"\\n   - If content cannot be associated with a valid number/identifier, ignore it.\"\n",
        "                        \"\\n   - Ensure the text following 'QUESTION X:' corresponds to the answer for that question.\"\n",
        "                        \"\\n   - Do not add any explanations, interpretations, or summaries.\"\n",
        "                        \"\\n   - Do not include any processing notes or thinking steps (like ◁think▷).\"\n",
        "                        \"\\n   - Remove any duplicate 'QUESTION X:' tags if they accidentally occur for the same X.\"\n",
        "                        \"\\nProcess all images together and provide a single coherent text output containing all the extracted answers, correctly labelled and combined.\"\n",
        "                    )\n",
        "                }\n",
        "            ] + image_contents # Append the list of image objects\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Make the API call\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=VL_MODEL_NAME,\n",
        "            messages=messages\n",
        "            # Consider adding max_tokens if needed, though VL models often handle this well\n",
        "            # max_tokens=4096 # Example\n",
        "        )\n",
        "        extracted_text = completion.choices[0].message.content\n",
        "\n",
        "        # Clean up the extracted text (same cleaning as original code)\n",
        "        # Rename to avoid potential future conflicts? -> clean_vl_output\n",
        "        # New - Renaming this function\n",
        "        def clean_vl_output(vl_text):\n",
        "            \"\"\"Cleans VL model specific output artifacts.\"\"\"\n",
        "            cleaned = vl_text\n",
        "            # Remove artifacts seen in original code\n",
        "            cleaned = cleaned.replace(\"◁think▷\", \"\")\n",
        "            cleaned = cleaned.replace(\"Valid Question Numbers:\", \"\") # Remove potential meta-text\n",
        "            cleaned = cleaned.replace(\"Question Paper Text:\", \"\")   # Remove potential meta-text\n",
        "            # Remove other common VL model preamble/postamble if observed\n",
        "            cleaned = re.sub(r\"^\\s*Here is the extracted text.*?\\n+\", \"\", cleaned, flags=re.IGNORECASE)\n",
        "            cleaned = re.sub(r\"\\n+Okay, I have processed the images.*$\", \"\", cleaned, flags=re.IGNORECASE | re.DOTALL)\n",
        "            cleaned = cleaned.strip()\n",
        "            # Ensure consistent spacing around QUESTION tags (add newline before if missing)\n",
        "            cleaned = re.sub(r'(?<!\\n)\\n?(QUESTION \\S+:)', r'\\n\\n\\1', cleaned)\n",
        "            cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned) # Consolidate newlines\n",
        "            return cleaned.strip()\n",
        "\n",
        "        cleaned_text = clean_vl_output(extracted_text)\n",
        "\n",
        "        print(f\"Answer text extracted successfully from {len(image_contents)} image(s).\")\n",
        "        return cleaned_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during VL API call for answer extraction: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "\n",
        "# From OCRandSegment.ipynb (preprocess_text - now specific to cleaning VL output artifacts)\n",
        "# Note: This was already specific, but renaming makes it clearer.\n",
        "# Kept original name here to match dependencies, but wrapped its logic in clean_vl_output above.\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the raw text extracted by the VL model to remove metadata/notes.\n",
        "    (This function's logic is now inside `clean_vl_output` called by `extract_text_from_images`)\n",
        "    \"\"\"\n",
        "    # The actual cleaning happens within extract_text_from_images now using clean_vl_output.\n",
        "    # This function remains mostly for compatibility if it was called elsewhere,\n",
        "    # but ideally, the output from extract_text_from_images is already cleaned.\n",
        "    # We can simply return the text as is, assuming it's cleaned by the caller.\n",
        "    if text is None: return None\n",
        "    # Or, apply the cleaning again just in case:\n",
        "    return text # Assuming extract_text_from_images already returns cleaned text.\n",
        "\n",
        "\n",
        "# From OCRandSegment.ipynb\n",
        "def convert_to_dict(text):\n",
        "    \"\"\"\n",
        "    Convert the structured VL text output (\"QUESTION X: ...\") into a dictionary.\n",
        "    Keys are integers, values are the corresponding answer strings.\n",
        "    Handles potential errors during parsing.\n",
        "    \"\"\"\n",
        "    qa_dict = {}\n",
        "    if not text:\n",
        "        print(\"Warning: Cannot convert empty text to dictionary.\")\n",
        "        return qa_dict\n",
        "\n",
        "    # Split text based on \"QUESTION X:\" pattern. Handles variations like \"QUESTION 1:\", \"QUESTION 3a:\"\n",
        "    # Positive lookahead `(?=...)` ensures the delimiter is not consumed.\n",
        "    # Handles potential whitespace variations.\n",
        "    questions = re.split(r'(?=QUESTION\\s+\\S+\\s*:)', text.strip(), flags=re.IGNORECASE)\n",
        "    questions = [q.strip() for q in questions if q.strip()] # Filter out empty strings\n",
        "\n",
        "    if not questions:\n",
        "         print(\"Warning: No 'QUESTION X:' patterns found in the text to split by.\")\n",
        "         # Maybe return the whole text under a default key?\n",
        "         # qa_dict[0] = text\n",
        "         return qa_dict\n",
        "\n",
        "    for entry in questions:\n",
        "        # Match \"QUESTION X:\" at the beginning of the string\n",
        "        match = re.match(r'QUESTION\\s+(\\S+)\\s*:(.*)', entry, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            question_id_str = match.group(1).strip()\n",
        "            answer = match.group(2).strip()\n",
        "            try:\n",
        "                # Convert key to integer - THIS IS CRUCIAL for matching marks/key dicts\n",
        "                question_id_int = int(question_id_str)\n",
        "                qa_dict[question_id_int] = answer\n",
        "            except ValueError:\n",
        "                print(f\"Warning: Could not convert question identifier '{question_id_str}' to integer. Storing as string key.\")\n",
        "                # Decide how to handle non-integer keys if they shouldn't occur.\n",
        "                # Option 1: Store as string (might break later matching)\n",
        "                qa_dict[question_id_str] = answer\n",
        "                # Option 2: Skip this entry\n",
        "                # print(f\"Skipping entry with non-integer identifier: {question_id_str}\")\n",
        "                # continue\n",
        "        else:\n",
        "             print(f\"Warning: Could not parse entry starting with: '{entry[:50]}...'\")\n",
        "\n",
        "\n",
        "    # Add a check for duplicate keys (if the VL model accidentally repeats a QUESTION tag)\n",
        "    # This implementation implicitly takes the *last* occurrence found due to dict assignment.\n",
        "\n",
        "    if not qa_dict:\n",
        "         print(\"Warning: Resulting dictionary is empty after parsing.\")\n",
        "\n",
        "    return qa_dict\n",
        "\n",
        "# New - Orchestration function for processing Answer Sheet (PDF or Image(s))\n",
        "def process_answer_sheet_file(answer_sheet_path, question_paper_text_context, image_output_dir):\n",
        "    \"\"\"\n",
        "    Handles PDF/Image input for answer sheet, performs OCR/Structuring via VL,\n",
        "    and returns the structured dictionary.\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    if isinstance(answer_sheet_path, str) and answer_sheet_path.lower().endswith(\".pdf\"):\n",
        "        image_paths = pdf_to_images(answer_sheet_path, output_folder=image_output_dir)\n",
        "    elif isinstance(answer_sheet_path, str) and os.path.isfile(answer_sheet_path): # Single image file\n",
        "        image_paths = [answer_sheet_path]\n",
        "    elif isinstance(answer_sheet_path, list): # List of image files\n",
        "        image_paths = answer_sheet_path\n",
        "    else:\n",
        "        print(f\"ERROR: Invalid answer_sheet_path format: {answer_sheet_path}. Provide PDF path, image path, or list of image paths.\")\n",
        "        return None\n",
        "\n",
        "    if not image_paths:\n",
        "        print(\"ERROR: No images found or generated for the answer sheet.\")\n",
        "        return None\n",
        "\n",
        "    # Extract text using VL model\n",
        "    extracted_answer_text = extract_text_from_images(image_paths, question_paper_text_context)\n",
        "\n",
        "    if not extracted_answer_text:\n",
        "        print(\"ERROR: Failed to extract text from answer sheet images.\")\n",
        "        return None\n",
        "\n",
        "    # Convert structured text to dictionary (ensure integer keys)\n",
        "    student_answers_dict = convert_to_dict(extracted_answer_text)\n",
        "\n",
        "    return student_answers_dict\n",
        "\n",
        "\n",
        "# New - Orchestration function for processing Answer Key File (PDF or Image(s))\n",
        "def process_key_file_to_dict(answer_key_path, question_paper_text_context, image_output_dir):\n",
        "    \"\"\"\n",
        "    Handles PDF/Image input for answer key, performs OCR/Structuring via VL,\n",
        "    and returns the structured dictionary {int: answer_string}.\n",
        "    (Very similar to process_answer_sheet_file)\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    if isinstance(answer_key_path, str) and answer_key_path.lower().endswith(\".pdf\"):\n",
        "        image_paths = pdf_to_images(answer_key_path, output_folder=image_output_dir)\n",
        "    elif isinstance(answer_key_path, str) and os.path.isfile(answer_key_path): # Single image file\n",
        "        image_paths = [answer_key_path]\n",
        "    elif isinstance(answer_key_path, list): # List of image files\n",
        "        image_paths = answer_key_path\n",
        "    else:\n",
        "        print(f\"ERROR: Invalid answer_key_path format: {answer_key_path}. Provide PDF path, image path, or list of image paths.\")\n",
        "        return None\n",
        "\n",
        "    if not image_paths:\n",
        "        print(\"ERROR: No images found or generated for the answer key.\")\n",
        "        return None\n",
        "\n",
        "    # Extract text using VL model - Use the same function, prompt should be general enough\n",
        "    # Might consider a slightly different prompt if keys need different handling, but unlikely.\n",
        "    print(\"\\n--- Processing Answer Key File ---\")\n",
        "    extracted_key_text = extract_text_from_images(image_paths, question_paper_text_context)\n",
        "\n",
        "    if not extracted_key_text:\n",
        "        print(\"ERROR: Failed to extract text from answer key file.\")\n",
        "        return None\n",
        "\n",
        "    # Convert structured text to dictionary (ensure integer keys)\n",
        "    reference_answers_dict = convert_to_dict(extracted_key_text)\n",
        "    print(\"Finished processing answer key file.\")\n",
        "\n",
        "    return reference_answers_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e00a84d-9fc3-46ea-82f1-168e20ae6619",
      "metadata": {
        "id": "6e00a84d-9fc3-46ea-82f1-168e20ae6619"
      },
      "outputs": [],
      "source": [
        "# Code Cell 5: Grading Logic and Evaluation Functions\n",
        "# ----------------------------------------------------\n",
        "# All functions from the relevant part of Marks_Accuracy_model1.ipynb\n",
        "\n",
        "# --- Answer Type Detection ---\n",
        "def detect_answer_type(reference_text):\n",
        "    \"\"\"Detect the type of answer (code, mathematical, table, text) to adjust scoring weights.\"\"\"\n",
        "    if not isinstance(reference_text, str): return \"text\" # Handle non-string input\n",
        "\n",
        "    # Compile regex patterns once (moved outside for efficiency, though function scope is ok)\n",
        "    code_pattern = re.compile(r'\\b(void|int|char|float|double|struct|class|def|import|while|for|if|else|return)\\b', re.IGNORECASE)\n",
        "    # More robust math pattern - includes common functions, greek letters (approximation), operators\n",
        "    math_pattern = re.compile(r'(\\$\\$.*?\\$\\$|\\\\[a-zA-Z]+|\\b(sin|cos|tan|log|exp|sqrt)\\b|[+\\-*/=<>≤≥^]|[ΣΠ∫]|[α-ωΑ-Ω])')\n",
        "    table_pattern = re.compile(r'[|\\+][-+]{3,}[|\\+]') # Requires at least 3 dashes for a line\n",
        "\n",
        "    # Check for code keywords (increase threshold slightly?)\n",
        "    if len(code_pattern.findall(reference_text)) > 4: # Original was > 3\n",
        "        return \"code\"\n",
        "\n",
        "    # Check for mathematical content (increase threshold slightly?)\n",
        "    if len(math_pattern.findall(reference_text)) > 6: # Original was > 5\n",
        "        return \"mathematical\"\n",
        "\n",
        "    # Check for tables (more robust check)\n",
        "    if table_pattern.search(reference_text) or reference_text.count('\\n|') > 1: # Look for multiple lines starting with |\n",
        "        return \"table\"\n",
        "\n",
        "    # Default\n",
        "    return \"text\"\n",
        "\n",
        "# --- Grading Weights ---\n",
        "# Define weights as constants (from original code)\n",
        "WEIGHTS = {\n",
        "    \"code\": {\n",
        "        'semantic_factual_similarity': 0.40,\n",
        "        'code_structure': 0.30, # Specific metric for code\n",
        "        'key_phrases': 0.10,\n",
        "        'length_appropriateness': 0.10,\n",
        "        'coherence': 0.05,\n",
        "        'sequence_alignment': 0.05\n",
        "    },\n",
        "    \"mathematical\": {\n",
        "        'semantic_factual_similarity': 0.50,\n",
        "        'mathematical_expressions': 0.30, # Specific metric for math\n",
        "        'key_phrases': 0.10,\n",
        "        'length_appropriateness': 0.05,\n",
        "        'coherence': 0.05\n",
        "        # Sequence alignment might be less relevant for math? Original didn't include it.\n",
        "    },\n",
        "    \"table\": {\n",
        "        'semantic_factual_similarity': 0.40,\n",
        "        'table_structure': 0.30, # Specific metric for tables\n",
        "        'key_phrases': 0.15,\n",
        "        'length_appropriateness': 0.10,\n",
        "        'coherence': 0.05\n",
        "    },\n",
        "    \"text\": {\n",
        "        'semantic_factual_similarity': 0.65, # Higher weight for general text\n",
        "        'length_appropriateness': 0.15,\n",
        "        'key_phrases': 0.10,\n",
        "        'coherence': 0.05,\n",
        "        'sequence_alignment': 0.05\n",
        "    },\n",
        "    # New: Handle error case gracefully\n",
        "    \"error\": {\n",
        "        'semantic_factual_similarity': 0.0,\n",
        "        'length_appropriateness': 0.0,\n",
        "        'key_phrases': 0.0,\n",
        "        'coherence': 0.0,\n",
        "        'sequence_alignment': 0.0\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_weights_by_answer_type(answer_type):\n",
        "    \"\"\"Return appropriate weights based on answer type.\"\"\"\n",
        "    return WEIGHTS.get(answer_type, WEIGHTS[\"text\"]) # Default to text weights if type unknown\n",
        "\n",
        "\n",
        "# --- Core Metric Calculation Functions ---\n",
        "\n",
        "# Function to wrap bert_score calculation for easier use and error handling\n",
        "# New - Standardized BERTScore function\n",
        "def calculate_bert_score(texts1, texts2, lang='en'):\n",
        "    \"\"\"Calculates BERTScore (P, R, F1) safely.\"\"\"\n",
        "    try:\n",
        "        # Ensure inputs are lists of strings\n",
        "        if isinstance(texts1, str): texts1 = [texts1]\n",
        "        if isinstance(texts2, str): texts2 = [texts2]\n",
        "        if not texts1 or not texts2 or not texts1[0] or not texts2[0]:\n",
        "             # Handle empty input cases\n",
        "             return 0.0, 0.0, 0.0\n",
        "\n",
        "        # device = 'cuda' if torch.cuda.is_available() else 'cpu' # Optional: specify device\n",
        "        P, R, F1 = bert_score.score(texts1, texts2, lang=lang, verbose=False, model_type='bert-base-uncased') # Specify model for consistency?\n",
        "        # Return mean scores if multiple pairs were processed, or single score otherwise\n",
        "        return P.mean().item(), R.mean().item(), F1.mean().item()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BERTScore: {e}\")\n",
        "        # print(f\"Texts involved: {texts1}, {texts2}\") # Debugging\n",
        "        return 0.0, 0.0, 0.0 # Return zero scores on error\n",
        "\n",
        "def get_semantic_factual_similarity(student_text, reference_text):\n",
        "    \"\"\"Calculate combined semantic (BERTScore F1) and factual (number match) similarity.\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    # Semantic Similarity using BERTScore F1\n",
        "    _, _, semantic_similarity_f1 = calculate_bert_score(student_text, reference_text)\n",
        "\n",
        "    # Factual Accuracy based on numbers\n",
        "    number_pattern = re.compile(r'\\b\\d+(?:\\.\\d+)?\\b') # Match standalone numbers\n",
        "    student_numbers = set(number_pattern.findall(student_text))\n",
        "    reference_numbers = set(number_pattern.findall(reference_text))\n",
        "\n",
        "    # Calculate intersection over union for numbers? Or just intersection / reference?\n",
        "    # Original used intersection / reference. Stick to that unless specified otherwise.\n",
        "    if not reference_numbers:\n",
        "        factual_accuracy = 1.0 if not student_numbers else 0.5 # Both empty = 1.0; Student has extra = lower score?\n",
        "    else:\n",
        "        intersection_count = len(student_numbers.intersection(reference_numbers))\n",
        "        factual_accuracy = intersection_count / len(reference_numbers)\n",
        "        # Penalize slightly if student introduces numbers not in reference? Optional.\n",
        "        # extra_numbers = len(student_numbers - reference_numbers)\n",
        "        # penalty = extra_numbers * 0.1\n",
        "        # factual_accuracy = max(0, factual_accuracy - penalty)\n",
        "\n",
        "    # Combine scores (original weights: 70% semantic, 30% factual)\n",
        "    combined_score = (semantic_similarity_f1 * 0.7 + factual_accuracy * 0.3)\n",
        "    return max(0.0, min(1.0, combined_score)) # Ensure score is within [0, 1]\n",
        "\n",
        "def check_length_ratio(student_text, reference_text):\n",
        "    \"\"\"Check if student answer length is appropriate relative to reference length (word count).\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    try:\n",
        "        student_length = len(word_tokenize(student_text))\n",
        "        reference_length = len(word_tokenize(reference_text))\n",
        "    except Exception as e:\n",
        "        print(f\"Error tokenizing for length ratio: {e}\")\n",
        "        return 0.0 # Return 0 if tokenization fails\n",
        "\n",
        "    if reference_length == 0:\n",
        "        return 0.0 if student_length > 0 else 1.0 # Perfect match if both empty, else 0\n",
        "\n",
        "    ratio = student_length / reference_length\n",
        "\n",
        "    # Original logic: score decreases if too long (beyond 1.5x) or too short.\n",
        "    if ratio <= 1.5 and ratio >= 0.5: # Allow some flexibility (e.g., within 50%-150%)\n",
        "         # Score proportional to ratio, capped at 1.0\n",
        "         # Map ratio [0.5, 1.5] to score [0.5, 1.0] (roughly) - simpler approach below\n",
        "         return min(1.0, ratio) if ratio <= 1.0 else (1.0 - (ratio - 1.0)*0.5) # Penalize length > 1.0\n",
        "    elif ratio < 0.5:\n",
        "        return ratio * 2 # Linear penalty for too short (0.5 ratio -> 1.0 score seems wrong, maybe just ratio?) Let's use ratio directly. --> return ratio\n",
        "        # Corrected approach: return ratio if too short.\n",
        "        return max(0.0, ratio) # Score is the ratio itself if too short\n",
        "    else: # ratio > 1.5\n",
        "        # Penalize excess length more sharply\n",
        "        return max(0.0, 1.0 - (ratio - 1.5) * 0.5) # Example penalty: score drops from 1 at 1.5x\n",
        "\n",
        "    # Simpler original logic interpretation:\n",
        "    # if ratio <= 1.5: return min(1.0, ratio) # Score = ratio, capped at 1 if student is shorter\n",
        "    # else: return min(1.0, 1.5 / ratio) # Score decreases inverse proportionally if student is longer\n",
        "\n",
        "    # Let's stick to the *second interpretation* of the original logic as it's simpler:\n",
        "    if reference_length == 0: return 1.0 if student_length == 0 else 0.0\n",
        "    ratio = student_length / reference_length\n",
        "    if ratio <= 1.0: # Student answer is shorter or equal\n",
        "        # Score is proportional to length, up to reference length\n",
        "        return ratio\n",
        "    elif ratio <= 1.5: # Student answer is slightly longer (up to 1.5x) - Full score? Or slight penalty?\n",
        "        # Original: min(1.0, 1.5/ratio) -> gives 1.0 at ratio=1.5. Let's use this.\n",
        "         return 1.0 # Allow up to 1.5x length without penalty? Let's try this.\n",
        "         # Original logic: return min(1.0, 1.5 / ratio) # Gives 1.0 at ratio 1.5, <1 above that.\n",
        "    else: # ratio > 1.5\n",
        "        # Score decreases\n",
        "        return max(0.0, min(1.0, 1.5 / ratio)) # Original logic was likely this\n",
        "\n",
        "\n",
        "def check_sequence_alignment(student_text, reference_text):\n",
        "    \"\"\"Check structural alignment (sentence and paragraph counts).\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    try:\n",
        "        student_sentences = sent_tokenize(student_text)\n",
        "        reference_sentences = sent_tokenize(reference_text)\n",
        "        # Handle potential tokenization issues leading to empty lists\n",
        "        if not student_sentences and not reference_sentences: return 1.0\n",
        "        if not student_sentences or not reference_sentences: return 0.0\n",
        "\n",
        "        # Avoid division by zero - use max length as denominator\n",
        "        max_sentences = max(len(student_sentences), len(reference_sentences))\n",
        "        sent_ratio = min(len(student_sentences), len(reference_sentences)) / max_sentences if max_sentences > 0 else 1.0\n",
        "\n",
        "        student_paragraphs = [p for p in student_text.split('\\n\\n') if p.strip()]\n",
        "        reference_paragraphs = [p for p in reference_text.split('\\n\\n') if p.strip()]\n",
        "        if not student_paragraphs and not reference_paragraphs: return 1.0\n",
        "        if not student_paragraphs or not reference_paragraphs: return 0.0\n",
        "\n",
        "        max_paragraphs = max(len(student_paragraphs), len(reference_paragraphs))\n",
        "        para_ratio = min(len(student_paragraphs), len(reference_paragraphs)) / max_paragraphs if max_paragraphs > 0 else 1.0\n",
        "\n",
        "        # Combine sentence and paragraph alignment scores (average)\n",
        "        return (sent_ratio + para_ratio) / 2\n",
        "    except Exception as e:\n",
        "        print(f\"Error during sequence alignment: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def get_phrases(text, max_phrase_length=3):\n",
        "    \"\"\"Extract n-grams (phrases) up to max_phrase_length efficiently.\"\"\"\n",
        "    if not isinstance(text, str): return set()\n",
        "    try:\n",
        "        words = word_tokenize(text.lower())\n",
        "    except Exception as e:\n",
        "        print(f\"Error tokenizing for phrases: {e}\")\n",
        "        return set()\n",
        "\n",
        "    phrases = set()\n",
        "    # Generate n-grams from 1 up to max_phrase_length\n",
        "    for n in range(1, max_phrase_length + 1):\n",
        "        for i in range(len(words) - n + 1):\n",
        "            phrases.add(\" \".join(words[i:i+n]))\n",
        "    return phrases\n",
        "\n",
        "def check_key_phrases(student_text, reference_text):\n",
        "    \"\"\"Check for presence of key phrases (including synonyms for single words within phrases).\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    # Extract phrases (using n-grams up to length 3, as per original get_phrases)\n",
        "    ref_phrases = get_phrases(reference_text, max_phrase_length=3)\n",
        "    student_phrases = get_phrases(student_text, max_phrase_length=3)\n",
        "\n",
        "    if not ref_phrases:\n",
        "        return 1.0 if not student_phrases else 0.5 # No key phrases in reference\n",
        "\n",
        "    # Expand reference phrases with synonyms (only for single-word phrases for efficiency, or based on original logic?)\n",
        "    # Original code expanded synonyms *within* phrases up to 3 words. Let's replicate that carefully.\n",
        "    expanded_ref_phrases = set(ref_phrases) # Start with original phrases\n",
        "    phrases_to_expand = [p for p in ref_phrases if len(p.split()) <= 3] # Limit synonym expansion\n",
        "\n",
        "    # This synonym expansion can be computationally expensive. Limit it.\n",
        "    MAX_EXPANSIONS_PER_PHRASE = 5\n",
        "    for phrase in phrases_to_expand:\n",
        "        words = phrase.split()\n",
        "        expansion_count = 0\n",
        "        for i, word in enumerate(words):\n",
        "            if expansion_count >= MAX_EXPANSIONS_PER_PHRASE: break\n",
        "            synonyms = get_synonyms(word) # Uses cached function\n",
        "            for synonym in synonyms:\n",
        "                if expansion_count >= MAX_EXPANSIONS_PER_PHRASE: break\n",
        "                if synonym != word: # Avoid adding the original word again\n",
        "                    new_words = words[:i] + [synonym] + words[i+1:]\n",
        "                    expanded_ref_phrases.add(\" \".join(new_words))\n",
        "                    expansion_count += 1\n",
        "\n",
        "\n",
        "    # Direct phrase match score (intersection over reference set)\n",
        "    intersection = student_phrases.intersection(expanded_ref_phrases)\n",
        "    phrase_match_score = len(intersection) / len(expanded_ref_phrases) if expanded_ref_phrases else 1.0\n",
        "\n",
        "    # Original code also included a BERTScore component for semantic phrase matching.\n",
        "    # It calculated BERTScore between student *text* and a sample of *reference phrases*.\n",
        "    # Let's replicate that part.\n",
        "\n",
        "    # Select a limited number of reference phrases for BERTScore comparison to manage computation\n",
        "    key_ref_phrases_for_bert = list(expanded_ref_phrases)[:20] # Limit to 20 representative phrases\n",
        "\n",
        "    if not key_ref_phrases_for_bert:\n",
        "         bert_match_score = 0.0 # Or 1.0 if student text is also empty? Assume 0.0\n",
        "    else:\n",
        "        # Calculate BERTScore F1 between the whole student answer and each key phrase\n",
        "        _, _, bert_f1_scores = calculate_bert_score([student_text] * len(key_ref_phrases_for_bert), key_ref_phrases_for_bert)\n",
        "        # Average the F1 scores? Or take max? Original used mean.\n",
        "        bert_match_score = bert_f1_scores # calculate_bert_score already returns mean item\n",
        "\n",
        "    # Combine direct match and semantic match (original weights: 40% direct, 60% semantic)\n",
        "    combined_score = (phrase_match_score * 0.4 + bert_match_score * 0.6)\n",
        "    return max(0.0, min(1.0, combined_score))\n",
        "\n",
        "\n",
        "def check_coherence(text):\n",
        "    \"\"\"Check coherence using BERTScore between adjacent sentences.\"\"\"\n",
        "    if not isinstance(text, str): return 0.0\n",
        "    try:\n",
        "        sentences = sent_tokenize(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error tokenizing for coherence: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "    if len(sentences) < 2:\n",
        "        return 1.0 # Single sentence or empty text is considered coherent\n",
        "\n",
        "    coherence_scores = []\n",
        "    # Batching might be complex here due to pairs. Process sequentially.\n",
        "    for i in range(len(sentences) - 1):\n",
        "        # Calculate F1 score between adjacent sentences\n",
        "        _, _, f1 = calculate_bert_score(sentences[i], sentences[i+1])\n",
        "        coherence_scores.append(f1)\n",
        "\n",
        "    # Average the scores\n",
        "    return sum(coherence_scores) / len(coherence_scores) if coherence_scores else 1.0\n",
        "\n",
        "\n",
        "# --- Type-Specific Metric Functions ---\n",
        "\n",
        "# Compile patterns outside functions for efficiency\n",
        "CODE_FUNCTION_PATTERN = re.compile(r'\\b(void|int|char|float|double|struct|def|class)\\s+(\\w+)\\s*\\(.*\\)\\s*{?', re.IGNORECASE)\n",
        "LOOP_PATTERN = re.compile(r'\\b(while|for)\\s*\\(.*\\)\\s*{?', re.IGNORECASE)\n",
        "CONDITIONAL_PATTERN = re.compile(r'\\b(if|else\\s+if|elif)\\s*\\(.*\\)\\s*{?', re.IGNORECASE)\n",
        "VAR_DECL_PATTERN = re.compile(r'\\b(int|char|float|double|struct|String|var|let|const)\\s+\\w+\\s*(?:=.*;|;)', re.IGNORECASE) # Broader language support\n",
        "MEMORY_ALLOC_PATTERN = re.compile(r'\\b(malloc|calloc|realloc|new)\\b', re.IGNORECASE)\n",
        "COMPLEXITY_PATTERN = re.compile(r'\\b(O|Θ|Ω)\\s*\\([^)]*\\)', re.IGNORECASE) # Simpler complexity regex\n",
        "FORMULA_PATTERN = re.compile(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\s*=[^;=\\n]+(?:[+\\-*/%^<>≤≥]|&&|\\|\\|)[^;=\\n]+') # Basic formula/assignment check\n",
        "TABLE_PATTERN = re.compile(r'[|\\+][-+]{3,}[|\\+]') # Requires 3+ dashes\n",
        "TABLE_ROW_PATTERN = re.compile(r'^\\s*\\|.*\\|\\s*$', re.MULTILINE) # Match lines starting and ending with |\n",
        "\n",
        "def analyze_code_structure(student_text, reference_text):\n",
        "    \"\"\"Analyze structural elements (functions, loops, conditionals, vars) in code.\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    def extract_code_elements(text):\n",
        "        elements = {}\n",
        "        # Function names (simple extraction)\n",
        "        elements['functions'] = set(m.group(2) for m in CODE_FUNCTION_PATTERN.finditer(text))\n",
        "        elements['loops'] = len(LOOP_PATTERN.findall(text))\n",
        "        elements['conditionals'] = len(CONDITIONAL_PATTERN.findall(text))\n",
        "        elements['variables'] = len(VAR_DECL_PATTERN.findall(text))\n",
        "        elements['memory_ops'] = len(MEMORY_ALLOC_PATTERN.findall(text))\n",
        "        return elements\n",
        "\n",
        "    ref_elements = extract_code_elements(reference_text)\n",
        "    student_elements = extract_code_elements(student_text)\n",
        "\n",
        "    scores = {}\n",
        "    # Function matching (Jaccard index for function names)\n",
        "    ref_funcs = ref_elements['functions']\n",
        "    student_funcs = student_elements['functions']\n",
        "    if not ref_funcs and not student_funcs: scores['functions'] = 1.0\n",
        "    elif not ref_funcs or not student_funcs: scores['functions'] = 0.0\n",
        "    else:\n",
        "        intersection = len(ref_funcs.intersection(student_funcs))\n",
        "        union = len(ref_funcs.union(student_funcs))\n",
        "        scores['functions'] = intersection / union if union > 0 else 1.0\n",
        "\n",
        "    # Compare counts for other elements (loops, conditionals, vars, memory) using ratio logic\n",
        "    for elem_type in ['loops', 'conditionals', 'variables', 'memory_ops']:\n",
        "        ref_count = ref_elements.get(elem_type, 0)\n",
        "        student_count = student_elements.get(elem_type, 0)\n",
        "\n",
        "        if ref_count == 0:\n",
        "            scores[elem_type] = 1.0 if student_count == 0 else 0.5 # Penalize extra elements\n",
        "        else:\n",
        "            ratio = student_count / ref_count\n",
        "            # Use similar logic to length ratio: score = ratio if < 1, 1 if <= 1.5, decreases > 1.5\n",
        "            if ratio <= 1.0: score = ratio\n",
        "            elif ratio <= 1.5: score = 1.0\n",
        "            else: score = max(0.0, 1.5 / ratio)\n",
        "            scores[elem_type] = score\n",
        "\n",
        "    # Average the scores for different elements\n",
        "    final_score = sum(scores.values()) / len(scores) if scores else 0.0\n",
        "    return max(0.0, min(1.0, final_score))\n",
        "\n",
        "\n",
        "def compare_mathematical_expressions(student_text, reference_text):\n",
        "    \"\"\"Extract and compare mathematical formulas and complexity notations.\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    def extract_math_elements(text):\n",
        "        elements = set()\n",
        "        # Complexity notations\n",
        "        elements.update(m.group(0).replace(\" \", \"\").lower() for m in COMPLEXITY_PATTERN.finditer(text))\n",
        "        # Basic formulas (normalize whitespace)\n",
        "        elements.update(re.sub(r'\\s+', '', m.group(0)).lower() for m in FORMULA_PATTERN.finditer(text))\n",
        "        # Add other specific patterns if needed (e.g., summations, integrals - complex)\n",
        "        return elements\n",
        "\n",
        "    ref_expressions = extract_math_elements(reference_text)\n",
        "    student_expressions = extract_math_elements(student_text)\n",
        "\n",
        "    if not ref_expressions:\n",
        "        return 1.0 if not student_expressions else 0.5 # No math expected\n",
        "\n",
        "    # Compare using Jaccard index on the extracted sets\n",
        "    intersection = len(student_expressions.intersection(ref_expressions))\n",
        "    union = len(student_expressions.union(ref_expressions))\n",
        "\n",
        "    return intersection / union if union > 0 else 1.0\n",
        "\n",
        "\n",
        "def analyze_table_structure(student_text, reference_text):\n",
        "    \"\"\"Analyze table structure similarity (presence, rows, cols, headers).\"\"\"\n",
        "    if not isinstance(student_text, str) or not isinstance(reference_text, str): return 0.0\n",
        "\n",
        "    # Basic check for table presence\n",
        "    student_has_table = bool(TABLE_PATTERN.search(student_text) or TABLE_ROW_PATTERN.search(student_text))\n",
        "    ref_has_table = bool(TABLE_PATTERN.search(reference_text) or TABLE_ROW_PATTERN.search(reference_text))\n",
        "\n",
        "    if not ref_has_table:\n",
        "        return 1.0 if not student_has_table else 0.5 # Reference has no table\n",
        "\n",
        "    if not student_has_table:\n",
        "        return 0.0 # Reference expects table, student didn't provide\n",
        "\n",
        "    # Compare Rows (count lines matching row pattern)\n",
        "    student_rows = len(TABLE_ROW_PATTERN.findall(student_text))\n",
        "    ref_rows = len(TABLE_ROW_PATTERN.findall(reference_text))\n",
        "    if ref_rows == 0: row_score = 1.0 if student_rows == 0 else 0.5\n",
        "    else: row_score = min(student_rows, ref_rows) / max(student_rows, ref_rows) if max(student_rows, ref_rows) > 0 else 1.0\n",
        "\n",
        "    # Compare Columns (count separators in the first row found) - simplified\n",
        "    student_first_row_match = TABLE_ROW_PATTERN.search(student_text)\n",
        "    ref_first_row_match = TABLE_ROW_PATTERN.search(reference_text)\n",
        "    student_cols = student_first_row_match.group(0).count('|') - 1 if student_first_row_match else 0\n",
        "    ref_cols = ref_first_row_match.group(0).count('|') - 1 if ref_first_row_match else 0\n",
        "    if ref_cols <= 0: col_score = 1.0 if student_cols <= 0 else 0.5\n",
        "    else: col_score = min(student_cols, ref_cols) / max(student_cols, ref_cols) if max(student_cols, ref_cols) > 0 else 1.0\n",
        "\n",
        "    # Compare Headers (optional, simple check if first row cells match somewhat)\n",
        "    # This part is complex and was basic in the original. Keep it simple or omit?\n",
        "    # Let's use a simple Jaccard on the first row cell contents.\n",
        "    header_score = 0.5 # Default if cannot compare\n",
        "    if student_first_row_match and ref_first_row_match:\n",
        "        student_cells = set(c.strip().lower() for c in student_first_row_match.group(0).split('|')[1:-1]) # Cells between pipes\n",
        "        ref_cells = set(c.strip().lower() for c in ref_first_row_match.group(0).split('|')[1:-1])\n",
        "        if not ref_cells and not student_cells: header_score = 1.0\n",
        "        elif not ref_cells or not student_cells: header_score = 0.0\n",
        "        else:\n",
        "             intersection = len(student_cells.intersection(ref_cells))\n",
        "             union = len(student_cells.union(ref_cells))\n",
        "             header_score = intersection / union if union > 0 else 1.0\n",
        "\n",
        "    # Combine scores (e.g., 40% rows, 30% cols, 30% headers)\n",
        "    final_score = (row_score * 0.4 + col_score * 0.3 + header_score * 0.3)\n",
        "    return max(0.0, min(1.0, final_score))\n",
        "\n",
        "\n",
        "# --- Single Answer Evaluation ---\n",
        "def evaluate_single_answer(student_text, reference_text, total_marks):\n",
        "    \"\"\"Evaluate a student's answer against a reference using multiple weighted metrics.\"\"\"\n",
        "    details = {} # To store individual metric scores\n",
        "\n",
        "    # Handle cases where either text is missing or not a string\n",
        "    if not isinstance(student_text, str) or not student_text.strip():\n",
        "        print(\"  - Student answer is empty or invalid.\")\n",
        "        # Assign score based on reference? If reference is also empty, maybe full marks?\n",
        "        if not isinstance(reference_text, str) or not reference_text.strip():\n",
        "             answer_type = \"text\" # or N/A?\n",
        "             final_score = total_marks # Both empty, assume correct if marks > 0\n",
        "             details = {'error': 'Both answers empty'}\n",
        "        else:\n",
        "             answer_type = detect_answer_type(reference_text)\n",
        "             final_score = 0.0 # Student empty, reference not empty\n",
        "             details = {'error': 'Student answer empty'}\n",
        "        return {'final_score': round(final_score, 2), 'answer_type': answer_type, 'details': details}\n",
        "\n",
        "    if not isinstance(reference_text, str) or not reference_text.strip():\n",
        "        print(\"  - Reference answer is empty or invalid.\")\n",
        "        # Student provided something, reference empty. Partial marks? Or zero?\n",
        "        answer_type = \"text\" # Detect type based on student answer?\n",
        "        final_score = 0.0 # Cannot grade against empty reference\n",
        "        details = {'error': 'Reference answer empty'}\n",
        "        return {'final_score': round(final_score, 2), 'answer_type': answer_type, 'details': details}\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Detect answer type based on REFERENCE answer\n",
        "        answer_type = detect_answer_type(reference_text)\n",
        "        print(f\"  - Detected Reference Type: {answer_type}\")\n",
        "\n",
        "        # Calculate core metrics applicable to all types\n",
        "        details['semantic_factual_similarity'] = get_semantic_factual_similarity(student_text, reference_text)\n",
        "        details['length_appropriateness'] = check_length_ratio(student_text, reference_text)\n",
        "        details['sequence_alignment'] = check_sequence_alignment(student_text, reference_text)\n",
        "        details['key_phrases'] = check_key_phrases(student_text, reference_text)\n",
        "        details['coherence'] = check_coherence(student_text) # Coherence of student answer\n",
        "\n",
        "        # Calculate type-specific metrics\n",
        "        if answer_type == \"code\":\n",
        "            details['code_structure'] = analyze_code_structure(student_text, reference_text)\n",
        "        elif answer_type == \"mathematical\":\n",
        "            details['mathematical_expressions'] = compare_mathematical_expressions(student_text, reference_text)\n",
        "        elif answer_type == \"table\":\n",
        "            details['table_structure'] = analyze_table_structure(student_text, reference_text)\n",
        "\n",
        "        # Get weights for the detected type\n",
        "        weights = get_weights_by_answer_type(answer_type)\n",
        "\n",
        "        # Calculate weighted score\n",
        "        weighted_score = 0.0\n",
        "        total_applied_weight = 0.0\n",
        "        print(\"  - Calculating weighted score:\")\n",
        "        for metric, score in details.items():\n",
        "            weight = weights.get(metric)\n",
        "            if weight is not None: # Check if this metric has a weight defined for this type\n",
        "                print(f\"    - {metric}: Score={score:.3f}, Weight={weight:.2f}, Contribution={score * weight:.3f}\")\n",
        "                weighted_score += score * weight\n",
        "                total_applied_weight += weight\n",
        "            else:\n",
        "                print(f\"    - {metric}: Score={score:.3f} (No weight defined for type '{answer_type}')\")\n",
        "\n",
        "\n",
        "        # Normalize score by the sum of weights actually applied\n",
        "        if total_applied_weight > 0:\n",
        "            normalized_score = weighted_score / total_applied_weight\n",
        "        else:\n",
        "            normalized_score = 0.0 # Avoid division by zero if no weights applied\n",
        "            print(\"  - Warning: Total applied weight is zero. Score set to 0.\")\n",
        "\n",
        "        # Ensure score is between 0 and 1\n",
        "        final_normalized_score = max(0.0, min(1.0, normalized_score))\n",
        "\n",
        "        # Scale score to total marks for the question\n",
        "        scaled_score = final_normalized_score * total_marks\n",
        "        # Ensure final score does not exceed total marks\n",
        "        final_score = min(scaled_score, float(total_marks))\n",
        "\n",
        "        print(f\"  - Final Normalized Score (0-1): {final_normalized_score:.3f}\")\n",
        "        print(f\"  - Final Score ({total_marks} marks): {final_score:.2f}\")\n",
        "\n",
        "        return {\n",
        "            'final_score': round(final_score, 2),\n",
        "            'answer_type': answer_type,\n",
        "            'details': {k: round(v, 3) for k, v in details.items()} # Store rounded details\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR evaluating answer: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return {\n",
        "            'final_score': 0.0,\n",
        "            'answer_type': 'error',\n",
        "            'details': {'error': str(e)}\n",
        "        }\n",
        "\n",
        "\n",
        "# --- Overall Evaluation Orchestration ---\n",
        "def evaluate_student_answers(student_answers: dict, reference_answers: dict, rubrics_marks: dict):\n",
        "    \"\"\"\n",
        "    Evaluate multiple student answers against reference answers using detailed metrics.\n",
        "    Ensures keys are integers for matching.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    total_score_obtained = 0.0\n",
        "    total_marks_possible = 0.0\n",
        "\n",
        "    print(\"\\n--- Starting Detailed Evaluation ---\")\n",
        "\n",
        "    # Standardize keys to integers for reliable matching\n",
        "    try:\n",
        "        student_answers_int = {int(k): v for k, v in student_answers.items()}\n",
        "        reference_answers_int = {int(k): v for k, v in reference_answers.items()}\n",
        "        rubrics_marks_int = {int(k): v for k, v in rubrics_marks.items()}\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR: Could not convert all keys in input dictionaries to integers. Please ensure keys are numeric. Details: {e}\")\n",
        "        return None # Cannot proceed if keys are not consistently numeric\n",
        "\n",
        "    # Use keys from the rubrics/marks dictionary as the canonical list of questions to grade\n",
        "    qids_to_grade = sorted(rubrics_marks_int.keys())\n",
        "    print(f\"Questions to be evaluated (based on marks provided): {qids_to_grade}\")\n",
        "\n",
        "    for qid in qids_to_grade:\n",
        "        print(f\"\\n--- Evaluating Question {qid} ---\")\n",
        "\n",
        "        student_answer = student_answers_int.get(qid)\n",
        "        reference_answer = reference_answers_int.get(qid)\n",
        "        question_marks = rubrics_marks_int.get(qid) # Already checked it exists\n",
        "\n",
        "        # Handle missing student or reference answer for a question defined in marks\n",
        "        if student_answer is None:\n",
        "            print(f\"  - Warning: No student answer found for question {qid}. Awarding 0 marks.\")\n",
        "            evaluation = {'final_score': 0.0, 'answer_type': 'missing', 'details': {'error': 'Student answer missing'}}\n",
        "        elif reference_answer is None:\n",
        "            print(f\"  - Warning: No reference answer found for question {qid}. Cannot grade. Awarding 0 marks.\")\n",
        "            evaluation = {'final_score': 0.0, 'answer_type': 'missing', 'details': {'error': 'Reference answer missing'}}\n",
        "        else:\n",
        "            # Evaluate the answer\n",
        "            evaluation = evaluate_single_answer(\n",
        "                student_answer,\n",
        "                reference_answer,\n",
        "                question_marks,\n",
        "                # Note: BERTScore calculation is now inside get_semantic_factual_similarity etc.\n",
        "            )\n",
        "\n",
        "        results[qid] = evaluation\n",
        "        total_score_obtained += evaluation.get('final_score', 0.0) # Add score, default to 0 if 'final_score' missing\n",
        "        total_marks_possible += question_marks\n",
        "\n",
        "    # Calculate overall percentage safely\n",
        "    percentage = round((total_score_obtained / total_marks_possible) * 100, 2) if total_marks_possible > 0 else 0.0\n",
        "\n",
        "    print(\"\\n--- Detailed Evaluation Complete ---\")\n",
        "\n",
        "    return {\n",
        "        'question_results': results,\n",
        "        'overall_score': round(total_score_obtained, 2),\n",
        "        'total_marks': total_marks_possible,\n",
        "        'percentage': percentage\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8881db-7c60-4935-93ca-be9aa334fc84",
      "metadata": {
        "id": "6f8881db-7c60-4935-93ca-be9aa334fc84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59c332d-5238-448f-e477-1cab8b84d548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "  Automated Assignment Grading System (v2)  \n",
            "==============================================\n",
            "\n",
            "[Step 1/5] Processing Question Paper...\n",
            "Extracting text from Question Paper: question_paper.jpeg using meta-llama/llama-4-maverick:free...\n",
            "Question paper text extracted successfully.\n",
            "Question Paper Text Snippet:\n",
            "------------------------------\n",
            "QUESTIONS:\n",
            "\n",
            "QUESTION 1:\n",
            "Explain asymptotic notations used to represent the time complexity. [03]\n",
            "\n",
            "QUESTION 2:\n",
            "Write a pseudo code to delete duplicate nodes from the unsorted Singly Linked List. Perform sorting on the resultant list and display the same. [07]\n",
            "\n",
            "OR\n",
            "\n",
            "Write an algorithm to perform the following operations on Circular Linked List and explain the same with an example.\n",
            "1. Insertion at the start\n",
            "2. Deletion of last node [07]\n",
            "\n",
            "QUESTION 3:\n",
            "Convert given INFIX expression to POSTFIX with the...\n",
            "------------------------------\n",
            "\n",
            "[Step 2/5] Processing Student Answer Sheet...\n",
            "Converting PDF 'Joel-D041.pdf' to images in 'temp_images_output'...\n",
            "Error converting PDF '/content/Joel-D041.pdf' to images: Unable to get page count. Is poppler installed and in PATH?\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pdf2image/pdf2image.py\", line 581, in pdfinfo_from_path\n",
            "    proc = Popen(command, env=env, stdout=PIPE, stderr=PIPE)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1026, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1955, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pdfinfo'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-abdbe2665a16>\", line 15, in pdf_to_images\n",
            "    images = convert_from_path(pdf_path)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pdf2image/pdf2image.py\", line 127, in convert_from_path\n",
            "    page_count = pdfinfo_from_path(\n",
            "                 ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pdf2image/pdf2image.py\", line 607, in pdfinfo_from_path\n",
            "    raise PDFInfoNotInstalledError(\n",
            "pdf2image.exceptions.PDFInfoNotInstalledError: Unable to get page count. Is poppler installed and in PATH?\n",
            "\n",
            "ERROR: No images found or generated for the answer sheet.\n",
            "ERROR: Failed to process student answer sheet. Cannot proceed.\n",
            "\n",
            "[Step 3/5] Loading/Processing Answer Key...\n",
            "Converting PDF 'Spam_AnswerKey.pdf' to images in 'temp_images_output'...\n",
            "Error converting PDF '/content/Spam_AnswerKey.pdf' to images: Unable to get page count. Is poppler installed and in PATH?\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pdf2image/pdf2image.py\", line 581, in pdfinfo_from_path\n",
            "    proc = Popen(command, env=env, stdout=PIPE, stderr=PIPE)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1026, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1955, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pdfinfo'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-abdbe2665a16>\", line 15, in pdf_to_images\n",
            "    images = convert_from_path(pdf_path)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pdf2image/pdf2image.py\", line 127, in convert_from_path\n",
            "    page_count = pdfinfo_from_path(\n",
            "                 ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pdf2image/pdf2image.py\", line 607, in pdfinfo_from_path\n",
            "    raise PDFInfoNotInstalledError(\n",
            "pdf2image.exceptions.PDFInfoNotInstalledError: Unable to get page count. Is poppler installed and in PATH?\n",
            "\n",
            "ERROR: No images found or generated for the answer key.\n",
            "ERROR: Failed to load or process answer key. Cannot proceed.\n",
            "\n",
            "[Step 4/5] Preparing Marks Rubric...\n",
            "Marks rubric loaded for 3 questions.\n",
            "\n",
            "[Step 5/5] Performing Evaluation...\n",
            "Skipping evaluation due to errors in previous steps.\n",
            "\n",
            "==============================================\n",
            "             Grading Results\n",
            "==============================================\n",
            "Evaluation could not be completed due to errors.\n",
            "\n",
            "==============================================\n",
            "              End of Report\n",
            "==============================================\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 6: Main Execution Workflow\n",
        "# -----------------------------------\n",
        "\n",
        "print(\"==============================================\")\n",
        "print(\"  Automated Assignment Grading System (v2)  \")\n",
        "print(\"==============================================\")\n",
        "\n",
        "# --- Preparations ---\n",
        "# Ensure API client is ready\n",
        "if not client:\n",
        "    print(\"ERROR: OpenRouter API client failed to initialize. Grading cannot proceed.\")\n",
        "    # Exit or raise error\n",
        "    # exit() # Uncomment to stop execution\n",
        "\n",
        "# Create temporary directory for images if needed\n",
        "os.makedirs(IMAGE_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Step 1: Process Question Paper ---\n",
        "print(\"\\n[Step 1/5] Processing Question Paper...\")\n",
        "question_paper_text = extract_text_from_question_paper(QUESTION_PAPER_PATH)\n",
        "\n",
        "if not question_paper_text:\n",
        "    print(\"ERROR: Failed to process question paper. Cannot proceed.\")\n",
        "    # exit() # Uncomment to stop execution\n",
        "else:\n",
        "    # Display snippet of extracted QP text (optional)\n",
        "    print(\"Question Paper Text Snippet:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(question_paper_text[:500] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- Step 2: Process Student Answer Sheet ---\n",
        "print(\"\\n[Step 2/5] Processing Student Answer Sheet...\")\n",
        "student_answers_dict = process_answer_sheet_file(\n",
        "    ANSWER_SHEET_PATH,\n",
        "    question_paper_text, # Provide QP text as context for VL model\n",
        "    IMAGE_OUTPUT_FOLDER\n",
        ")\n",
        "\n",
        "if not student_answers_dict:\n",
        "    print(\"ERROR: Failed to process student answer sheet. Cannot proceed.\")\n",
        "    # exit() # Uncomment to stop execution\n",
        "else:\n",
        "    print(f\"Successfully processed student answer sheet. Found answers for {len(student_answers_dict)} questions.\")\n",
        "    # Display snippet (optional)\n",
        "    # print(\"Sample Student Answer (Q1):\", student_answers_dict.get(1, \"N/A\")[:200] + \"...\")\n",
        "\n",
        "\n",
        "# --- Step 3: Load or Process Answer Key ---\n",
        "print(\"\\n[Step 3/5] Loading/Processing Answer Key...\")\n",
        "final_reference_answers_dict = None\n",
        "\n",
        "if ANSWER_KEY_SOURCE == 'file':\n",
        "    final_reference_answers_dict = process_key_file_to_dict(\n",
        "        ANSWER_KEY_PATH,\n",
        "        question_paper_text, # Provide QP context here too if needed\n",
        "        IMAGE_OUTPUT_FOLDER\n",
        "    )\n",
        "elif ANSWER_KEY_SOURCE == 'dict':\n",
        "    # Ensure keys are integers if using predefined dict\n",
        "    try:\n",
        "        final_reference_answers_dict = {int(k): v for k, v in ANSWER_KEY_DICT.items()}\n",
        "        print(\"Using predefined answer key dictionary.\")\n",
        "    except ValueError:\n",
        "        print(\"ERROR: Keys in predefined ANSWER_KEY_DICT must be integers.\")\n",
        "        final_reference_answers_dict = None\n",
        "else:\n",
        "    print(f\"ERROR: Invalid ANSWER_KEY_SOURCE '{ANSWER_KEY_SOURCE}'. Use 'file' or 'dict'.\")\n",
        "\n",
        "if not final_reference_answers_dict:\n",
        "    print(\"ERROR: Failed to load or process answer key. Cannot proceed.\")\n",
        "    # exit() # Uncomment to stop execution\n",
        "else:\n",
        "     print(f\"Answer key ready. Contains answers for {len(final_reference_answers_dict)} questions.\")\n",
        "     # Display snippet (optional)\n",
        "     # print(\"Sample Reference Answer (Q1):\", final_reference_answers_dict.get(1, \"N/A\")[:200] + \"...\")\n",
        "\n",
        "\n",
        "# --- Step 4: Prepare Marks Dictionary ---\n",
        "print(\"\\n[Step 4/5] Preparing Marks Rubric...\")\n",
        "# Ensure keys are integers\n",
        "try:\n",
        "    final_marks_dict = {int(k): v for k, v in MARKS_PER_QUESTION.items()}\n",
        "    print(f\"Marks rubric loaded for {len(final_marks_dict)} questions.\")\n",
        "except ValueError:\n",
        "    print(\"ERROR: Keys in MARKS_PER_QUESTION must be integers.\")\n",
        "    final_marks_dict = None\n",
        "\n",
        "if not final_marks_dict:\n",
        "     print(\"ERROR: Failed to load marks rubric. Cannot proceed.\")\n",
        "     # exit()\n",
        "\n",
        "# --- Step 5: Perform Grading ---\n",
        "print(\"\\n[Step 5/5] Performing Evaluation...\")\n",
        "evaluation_results = None\n",
        "if student_answers_dict is not None and final_reference_answers_dict is not None and final_marks_dict is not None:\n",
        "    evaluation_results = evaluate_student_answers(\n",
        "        student_answers=student_answers_dict,\n",
        "        reference_answers=final_reference_answers_dict,\n",
        "        rubrics_marks=final_marks_dict\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping evaluation due to errors in previous steps.\")\n",
        "\n",
        "\n",
        "# --- Step 6: Display Results ---\n",
        "print(\"\\n==============================================\")\n",
        "print(\"             Grading Results\")\n",
        "print(\"==============================================\")\n",
        "\n",
        "if evaluation_results:\n",
        "    print(f\"\\nOverall Score: {evaluation_results['overall_score']} / {evaluation_results['total_marks']}\")\n",
        "    print(f\"Percentage: {evaluation_results['percentage']}%\")\n",
        "    print(\"\\n--- Scores per Question ---\")\n",
        "    for qid, result in sorted(evaluation_results['question_results'].items()):\n",
        "        marks = final_marks_dict.get(qid, \"N/A\")\n",
        "        print(f\"\\nQuestion {qid} (Max Marks: {marks}, Type: {result.get('answer_type', 'N/A')})\")\n",
        "        print(f\"  Score Awarded: {result.get('final_score', 'Error')}\")\n",
        "        print(\"  Details:\")\n",
        "        if 'details' in result and isinstance(result['details'], dict):\n",
        "            for metric, score in result['details'].items():\n",
        "                if metric != 'error': # Don't reprint error here\n",
        "                     print(f\"    - {metric:<30}: {score:.3f}\")\n",
        "                else:\n",
        "                     print(f\"    - Error during evaluation: {score}\")\n",
        "        elif 'final_score' in result and result['final_score'] == 0.0:\n",
        "             print(\"    - Evaluation skipped or failed (Score 0).\")\n",
        "        else:\n",
        "             print(\"    - No details available.\")\n",
        "\n",
        "else:\n",
        "    print(\"Evaluation could not be completed due to errors.\")\n",
        "\n",
        "print(\"\\n==============================================\")\n",
        "print(\"              End of Report\")\n",
        "print(\"==============================================\")\n",
        "\n",
        "# Optional: Clean up temporary image folder\n",
        "# import shutil\n",
        "# if os.path.exists(IMAGE_OUTPUT_FOLDER):\n",
        "#     print(f\"\\nCleaning up temporary image folder: {IMAGE_OUTPUT_FOLDER}\")\n",
        "#     # shutil.rmtree(IMAGE_OUTPUT_FOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff852b9-93bd-4fe1-80b2-0b9a117c7b8b",
      "metadata": {
        "id": "2ff852b9-93bd-4fe1-80b2-0b9a117c7b8b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}